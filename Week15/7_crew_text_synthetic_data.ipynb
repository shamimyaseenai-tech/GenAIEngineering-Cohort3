{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legal Document Generator POC with CrewAI + Serper\n",
    "\n",
    "This notebook generates synthetic legal documents using CrewAI with web research capabilities via Serper API.\n",
    "Documents are saved as individual TXT files for easy model training.\n",
    "**Note: All generated documents are synthetic and for training purposes only.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "\n",
    "! pip install crewai crewai-tools langchain-openai python-dotenv faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "from faker import Faker\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai_tools import SerperDevTool, FileReadTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize tools\n",
    "fake = Faker()\n",
    "search_tool = SerperDevTool()\n",
    "file_read_tool = FileReadTool()\n",
    "\n",
    "# Configure LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Verify API keys\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\")\n",
    "# os.environ[\"SERPER_API_KEY\"] = os.environ.get(\"SERPER_API_KEY\")\n",
    "# llm = LLM(model='openai/gpt-4o',api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPEN_ROUTER_KEY\")\n",
    "os.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\")\n",
    "# os.environ['HUGGINGFACEHUB_API_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "# os.environ['LITELLM_LOG'] = 'DEBUG'\n",
    "os.environ['OPENAI_API_BASE'] = 'https://openrouter.ai/api/v1'\n",
    "os.environ['OPENAI_BASE_URL'] = 'https://openrouter.ai/api/v1'\n",
    "\n",
    "\n",
    "\n",
    "print(\"‚úì Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Management Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocumentFileManager:\n",
    "    \"\"\"\n",
    "    Manages file operations for legal document generation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir=\"legal_documents\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.setup_directories()\n",
    "    \n",
    "    def setup_directories(self):\n",
    "        \"\"\"Create directory structure for legal documents\"\"\"\n",
    "        directories = [\n",
    "            'complaints',\n",
    "            'motions', \n",
    "            'contracts',\n",
    "            'memos',\n",
    "            'briefs',\n",
    "            'agreements',\n",
    "            'templates',\n",
    "            'metadata'\n",
    "        ]\n",
    "        \n",
    "        for dir_name in directories:\n",
    "            dir_path = self.base_dir / dir_name\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"‚úì Directory structure created in: {self.base_dir}\")\n",
    "    \n",
    "    def save_document_txt(self, content, doc_type, metadata=None):\n",
    "        \"\"\"Save document as TXT file with metadata\"\"\"\n",
    "        \n",
    "        # Generate filename with timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        case_id = metadata.get('case_number', f\"DOC_{random.randint(1000, 9999)}\") if metadata else f\"DOC_{random.randint(1000, 9999)}\"\n",
    "        safe_case_id = case_id.replace('-', '_').replace(' ', '_')\n",
    "        \n",
    "        filename = f\"{safe_case_id}_{timestamp}.txt\"\n",
    "        file_path = self.base_dir / doc_type / filename\n",
    "        \n",
    "        # Prepare content with header\n",
    "        header = f\"\"\"\n",
    "========================================\n",
    "SYNTHETIC LEGAL DOCUMENT - FOR TRAINING ONLY\n",
    "========================================\n",
    "Document Type: {doc_type.upper()}\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Document ID: {case_id}\n",
    "========================================\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        footer = f\"\"\"\n",
    "\n",
    "========================================\n",
    "END OF SYNTHETIC DOCUMENT\n",
    "This document was generated for AI training purposes.\n",
    "Do not use for actual legal proceedings.\n",
    "========================================\n",
    "\"\"\"\n",
    "        \n",
    "        full_content = header + content + footer\n",
    "        \n",
    "        # Save document\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(full_content)\n",
    "        \n",
    "        # Save metadata separately\n",
    "        if metadata:\n",
    "            metadata_filename = f\"{safe_case_id}_{timestamp}_metadata.json\"\n",
    "            metadata_path = self.base_dir / 'metadata' / metadata_filename\n",
    "            \n",
    "            with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(metadata, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"‚úì Saved: {file_path}\")\n",
    "        return str(file_path)\n",
    "    \n",
    "    def get_document_count(self):\n",
    "        \"\"\"Get count of generated documents by type\"\"\"\n",
    "        counts = {}\n",
    "        \n",
    "        for subdir in self.base_dir.iterdir():\n",
    "            if subdir.is_dir() and subdir.name != 'metadata':\n",
    "                txt_files = list(subdir.glob('*.txt'))\n",
    "                counts[subdir.name] = len(txt_files)\n",
    "        \n",
    "        return counts\n",
    "    \n",
    "    def clean_documents(self, doc_type=None):\n",
    "        \"\"\"Clean generated documents\"\"\"\n",
    "        if doc_type:\n",
    "            target_dir = self.base_dir / doc_type\n",
    "            if target_dir.exists():\n",
    "                shutil.rmtree(target_dir)\n",
    "                target_dir.mkdir()\n",
    "                print(f\"‚úì Cleaned {doc_type} documents\")\n",
    "        else:\n",
    "            shutil.rmtree(self.base_dir)\n",
    "            self.setup_directories()\n",
    "            print(\"‚úì Cleaned all documents\")\n",
    "\n",
    "# Initialize file manager\n",
    "file_manager = LegalDocumentFileManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Research for Legal Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_legal_templates(document_type):\n",
    "    \"\"\"\n",
    "    Research legal document templates and examples using Serper\n",
    "    \"\"\"\n",
    "    \n",
    "    search_queries = {\n",
    "        'complaint': [\n",
    "            \"civil complaint template format legal document\",\n",
    "            \"federal court complaint example structure\",\n",
    "            \"civil litigation complaint format sections\"\n",
    "        ],\n",
    "        'motion': [\n",
    "            \"motion to dismiss template legal format\",\n",
    "            \"summary judgment motion example structure\",\n",
    "            \"federal rules civil procedure motion format\"\n",
    "        ],\n",
    "        'contract': [\n",
    "            \"commercial contract template clauses\",\n",
    "            \"service agreement legal format example\",\n",
    "            \"business contract standard provisions\"\n",
    "        ],\n",
    "        'memo': [\n",
    "            \"legal memorandum format template structure\",\n",
    "            \"law firm internal memo example\",\n",
    "            \"legal analysis memorandum format\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    research_results = []\n",
    "    \n",
    "    if document_type in search_queries:\n",
    "        for query in search_queries[document_type]:\n",
    "            try:\n",
    "                # Using search_tool to find relevant information\n",
    "                result = search_tool.run(query)\n",
    "                research_results.append({\n",
    "                    'query': query,\n",
    "                    'results': result\n",
    "                })\n",
    "                print(f\"‚úì Researched: {query}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Search failed for '{query}': {e}\")\n",
    "    \n",
    "    return research_results\n",
    "\n",
    "def extract_template_insights(research_results):\n",
    "    \"\"\"\n",
    "    Extract key insights from research results\n",
    "    \"\"\"\n",
    "    \n",
    "    insights = {\n",
    "        'common_sections': [],\n",
    "        'formatting_notes': [],\n",
    "        'legal_language_patterns': []\n",
    "    }\n",
    "    \n",
    "    # Process research results to extract patterns\n",
    "    for result in research_results:\n",
    "        if isinstance(result.get('results'), str):\n",
    "            content = result['results'].lower()\n",
    "            \n",
    "            # Look for common legal document sections\n",
    "            section_keywords = [\n",
    "                'caption', 'jurisdiction', 'venue', 'parties', 'facts',\n",
    "                'causes of action', 'prayer for relief', 'signature',\n",
    "                'whereas', 'recitals', 'definitions', 'termination'\n",
    "            ]\n",
    "            \n",
    "            for keyword in section_keywords:\n",
    "                if keyword in content:\n",
    "                    insights['common_sections'].append(keyword)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    insights['common_sections'] = list(set(insights['common_sections']))\n",
    "    \n",
    "    return insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Data Generation with Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_enhanced_case_data():\n",
    "    \"\"\"Generate enhanced synthetic case data\"\"\"\n",
    "    \n",
    "    case_types = [\n",
    "        \"Contract Dispute\", \"Personal Injury\", \"Employment Discrimination\", \n",
    "        \"Real Estate Litigation\", \"Business Tort\", \"Breach of Fiduciary Duty\",\n",
    "        \"Intellectual Property Infringement\", \"Securities Fraud\", \"Insurance Bad Faith\",\n",
    "        \"Product Liability\", \"Professional Malpractice\", \"Construction Defect\"\n",
    "    ]\n",
    "    \n",
    "    courts = [\n",
    "        f\"{fake.city()} County Superior Court\",\n",
    "        f\"United States District Court for the {fake.state()} District\",\n",
    "        f\"{fake.state()} Court of Appeals\",\n",
    "        f\"{fake.city()} Municipal Court\"\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"case_number\": f\"CV-{random.randint(2020, 2024)}-{random.randint(10000, 99999)}\",\n",
    "        \"case_type\": random.choice(case_types),\n",
    "        \"plaintiff\": fake.company() if random.choice([True, False]) else fake.name(),\n",
    "        \"defendant\": fake.company() if random.choice([True, False]) else fake.name(),\n",
    "        \"plaintiff_attorney\": fake.name(),\n",
    "        \"defendant_attorney\": fake.name(),\n",
    "        \"court\": random.choice(courts),\n",
    "        \"judge\": f\"Hon. {fake.name()}\",\n",
    "        \"filing_date\": fake.date_between(start_date='-2y', end_date='today'),\n",
    "        \"amount_in_controversy\": random.randint(25000, 10000000),\n",
    "        \"law_firm_plaintiff\": f\"{fake.last_name()}, {fake.last_name()} & Associates LLP\",\n",
    "        \"law_firm_defendant\": f\"{fake.last_name()} Law Group PC\",\n",
    "        \"address_plaintiff\": fake.address(),\n",
    "        \"address_defendant\": fake.address(),\n",
    "        \"jurisdiction\": fake.state(),\n",
    "        \"cause_of_action\": random.choice([\n",
    "            \"Breach of Contract\", \"Negligence\", \"Fraud\", \"Unjust Enrichment\",\n",
    "            \"Conversion\", \"Defamation\", \"Intentional Infliction of Emotional Distress\"\n",
    "        ])\n",
    "    }\n",
    "\n",
    "def generate_enhanced_contract_data():\n",
    "    \"\"\"Generate enhanced synthetic contract data\"\"\"\n",
    "    \n",
    "    contract_types = [\n",
    "        \"Software License Agreement\", \"Master Service Agreement\", \"Non-Disclosure Agreement\",\n",
    "        \"Asset Purchase Agreement\", \"Commercial Lease Agreement\", \"Joint Venture Agreement\",\n",
    "        \"Distribution Agreement\", \"Employment Agreement\", \"Consulting Agreement\",\n",
    "        \"Supply Agreement\", \"Franchise Agreement\", \"Technology Transfer Agreement\"\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"contract_type\": random.choice(contract_types),\n",
    "        \"party_a\": fake.company(),\n",
    "        \"party_b\": fake.company() if random.choice([True, False]) else fake.name(),\n",
    "        \"effective_date\": fake.date_between(start_date='-6m', end_date='+1m'),\n",
    "        \"termination_date\": fake.date_between(start_date='+1y', end_date='+5y'),\n",
    "        \"contract_value\": random.randint(10000, 5000000),\n",
    "        \"governing_law\": fake.state(),\n",
    "        \"jurisdiction\": f\"{fake.city()}, {fake.state()}\",\n",
    "        \"payment_terms\": random.choice([\n",
    "            \"Net 30 days\", \"Net 60 days\", \"Payment upon delivery\", \n",
    "            \"Monthly installments\", \"Quarterly payments\"\n",
    "        ]),\n",
    "        \"renewal_terms\": random.choice([\n",
    "            \"Automatic renewal\", \"Mutual consent\", \"30-day notice required\"\n",
    "        ])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced CrewAI Agents with Research Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced agents with research capabilities\n",
    "\n",
    "research_agent = Agent(\n",
    "    role=\"Legal Research Specialist\",\n",
    "    goal=\"Research legal document templates and best practices using web search\",\n",
    "    backstory=\"\"\"You are a legal research specialist who uses web search to find \n",
    "    current legal document templates, formatting standards, and best practices. \n",
    "    You analyze multiple sources to understand proper legal document structure.\"\"\",\n",
    "    tools=[search_tool],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "litigation_attorney = Agent(\n",
    "    role=\"Senior Litigation Attorney\",\n",
    "    goal=\"Generate comprehensive and realistic litigation documents based on research findings\",\n",
    "    backstory=\"\"\"You are a senior litigation attorney with 20+ years of experience \n",
    "    in federal and state courts. You create documents that follow current legal \n",
    "    standards and incorporate best practices from recent case examples.\"\"\",\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "contract_specialist = Agent(\n",
    "    role=\"Corporate Contract Specialist\",\n",
    "    goal=\"Create detailed and legally sound contract documents with modern commercial terms\",\n",
    "    backstory=\"\"\"You are a corporate contract specialist with expertise in \n",
    "    commercial transactions. You draft contracts that reflect current market \n",
    "    practices and include comprehensive risk management provisions.\"\"\",\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "document_reviewer = Agent(\n",
    "    role=\"Legal Document Quality Reviewer\",\n",
    "    goal=\"Review and enhance legal documents for accuracy, completeness, and professional standards\",\n",
    "    backstory=\"\"\"You are a meticulous legal document reviewer who ensures all \n",
    "    documents meet the highest professional standards, include proper legal \n",
    "    citations, and follow jurisdictional requirements.\"\"\",\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "file_manager_agent = Agent(\n",
    "    role=\"Document Management Specialist\",\n",
    "    goal=\"Format and save legal documents as properly structured text files\",\n",
    "    backstory=\"\"\"You are a document management specialist who ensures all \n",
    "    generated legal documents are properly formatted, saved with appropriate \n",
    "    metadata, and organized for training purposes.\"\"\",\n",
    "    tools=[file_read_tool],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Document Generation Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_research_task(document_type):\n",
    "    \"\"\"Create a research task for legal document templates\"\"\"\n",
    "    \n",
    "    return Task(\n",
    "        description=f\"\"\"\n",
    "        Research current legal document templates and formatting standards for {document_type} documents.\n",
    "        \n",
    "        Find information about:\n",
    "        1. Standard document structure and sections\n",
    "        2. Required legal formatting and citation styles\n",
    "        3. Current best practices and conventions\n",
    "        4. Jurisdictional requirements and variations\n",
    "        5. Common clauses and legal language patterns\n",
    "        \n",
    "        Search for examples from reputable legal sources, court websites, \n",
    "        and legal practice guides. Focus on finding templates that show \n",
    "        proper structure and professional formatting.\n",
    "        \n",
    "        Provide a comprehensive summary of findings that can guide document generation.\n",
    "        \"\"\",\n",
    "        agent=research_agent,\n",
    "        expected_output=f\"Comprehensive research summary on {document_type} document standards and templates\"\n",
    "    )\n",
    "\n",
    "def create_enhanced_complaint_task(case_data, research_context=\"\"):\n",
    "    return Task(\n",
    "        description=f\"\"\"\n",
    "        Draft a comprehensive civil complaint incorporating research findings and best practices.\n",
    "        \n",
    "        Case Details:\n",
    "        - Case Number: {case_data['case_number']}\n",
    "        - Case Type: {case_data['case_type']}\n",
    "        - Plaintiff: {case_data['plaintiff']}\n",
    "        - Defendant: {case_data['defendant']}\n",
    "        - Court: {case_data['court']}\n",
    "        - Amount in Controversy: ${case_data['amount_in_controversy']:,}\n",
    "        - Cause of Action: {case_data['cause_of_action']}\n",
    "        \n",
    "        Research Context: {research_context}\n",
    "        \n",
    "        Create a complete complaint with:\n",
    "        1. Proper caption with case styling\n",
    "        2. Parties section with detailed identification\n",
    "        3. Jurisdiction and venue allegations\n",
    "        4. Factual background (numbered paragraphs)\n",
    "        5. Multiple causes of action with elements\n",
    "        6. Damages allegations\n",
    "        7. Prayer for relief with specific requests\n",
    "        8. Jury demand\n",
    "        9. Verification clause\n",
    "        10. Attorney signature block with bar number\n",
    "        \n",
    "        Include realistic legal citations and follow current formatting standards.\n",
    "        Make content detailed and professional while keeping it clearly synthetic.\n",
    "        \"\"\",\n",
    "        agent=litigation_attorney,\n",
    "        expected_output=\"A complete, professionally formatted civil complaint ready for filing\"\n",
    "    )\n",
    "\n",
    "def create_enhanced_contract_task(contract_data, research_context=\"\"):\n",
    "    return Task(\n",
    "        description=f\"\"\"\n",
    "        Draft a comprehensive {contract_data['contract_type']} incorporating modern commercial practices.\n",
    "        \n",
    "        Contract Details:\n",
    "        - Contract Type: {contract_data['contract_type']}\n",
    "        - Party A: {contract_data['party_a']}\n",
    "        - Party B: {contract_data['party_b']}\n",
    "        - Contract Value: ${contract_data['contract_value']:,}\n",
    "        - Effective Date: {contract_data['effective_date']}\n",
    "        - Term: {contract_data['termination_date']}\n",
    "        - Governing Law: {contract_data['governing_law']}\n",
    "        - Payment Terms: {contract_data['payment_terms']}\n",
    "        \n",
    "        Research Context: {research_context}\n",
    "        \n",
    "        Include comprehensive sections:\n",
    "        1. Cover page with contract title and parties\n",
    "        2. Table of contents (for longer contracts)\n",
    "        3. Preamble with party identification\n",
    "        4. Recitals explaining transaction background\n",
    "        5. Definitions section with key terms\n",
    "        6. Main operative provisions and obligations\n",
    "        7. Payment and financial terms\n",
    "        8. Performance standards and deliverables\n",
    "        9. Intellectual property provisions\n",
    "        10. Confidentiality and non-disclosure\n",
    "        11. Termination and breach provisions\n",
    "        12. Dispute resolution mechanisms\n",
    "        13. Force majeure and risk allocation\n",
    "        14. Governing law and jurisdiction\n",
    "        15. General provisions (integration, amendments, etc.)\n",
    "        16. Signature pages with execution details\n",
    "        \n",
    "        Make the contract comprehensive and commercially realistic.\n",
    "        \"\"\",\n",
    "        agent=contract_specialist,\n",
    "        expected_output=f\"A complete, professionally drafted {contract_data['contract_type']}\"\n",
    "    )\n",
    "\n",
    "def create_review_task():\n",
    "    return Task(\n",
    "        description=\"\"\"\n",
    "        Review the generated legal document for:\n",
    "        \n",
    "        1. Legal accuracy and completeness\n",
    "        2. Professional formatting and structure\n",
    "        3. Proper legal citations and references\n",
    "        4. Consistency in terminology and style\n",
    "        5. Inclusion of all required legal elements\n",
    "        6. Appropriate disclaimers and notices\n",
    "        7. Compliance with document type standards\n",
    "        \n",
    "        Enhance the document by:\n",
    "        - Adding any missing standard provisions\n",
    "        - Improving legal language and precision\n",
    "        - Ensuring proper paragraph numbering\n",
    "        - Verifying citation formats\n",
    "        - Adding synthetic case law references where appropriate\n",
    "        \n",
    "        Provide a final, polished version ready for file output.\n",
    "        \"\"\",\n",
    "        agent=document_reviewer,\n",
    "        expected_output=\"A polished, professional legal document ready for training use\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Document Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_legal_document_with_research(document_type, num_documents=1, use_research=True):\n",
    "    \"\"\"\n",
    "    Generate legal documents with web research and save as TXT files\n",
    "    \n",
    "    Args:\n",
    "        document_type: Type of document ('complaint', 'motion', 'contract', 'memo')\n",
    "        num_documents: Number of documents to generate\n",
    "        use_research: Whether to perform web research first\n",
    "    \n",
    "    Returns:\n",
    "        List of generated file paths\n",
    "    \"\"\"\n",
    "    \n",
    "    generated_files = []\n",
    "    research_context = \"\"\n",
    "    \n",
    "    # Perform research if requested\n",
    "    if use_research:\n",
    "        print(f\"üîç Researching {document_type} templates and standards...\")\n",
    "        \n",
    "        research_task = create_research_task(document_type)\n",
    "        research_crew = Crew(\n",
    "            agents=[research_agent],\n",
    "            tasks=[research_task],\n",
    "            process=Process.sequential,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            research_result = research_crew.kickoff()\n",
    "            research_context = str(research_result)[:2000]  # Limit context length\n",
    "            print(\"‚úì Research completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Research failed: {e}\")\n",
    "            research_context = \"No research context available\"\n",
    "    \n",
    "    # Generate documents\n",
    "    for i in range(num_documents):\n",
    "        print(f\"\\nüìù Generating {document_type} {i+1}/{num_documents}...\")\n",
    "        \n",
    "        try:\n",
    "            # Generate case or contract data\n",
    "            if document_type in ['complaint', 'motion', 'memo']:\n",
    "                case_data = generate_enhanced_case_data()\n",
    "                metadata = case_data\n",
    "                \n",
    "                if document_type == 'complaint':\n",
    "                    main_task = create_enhanced_complaint_task(case_data, research_context)\n",
    "                elif document_type == 'motion':\n",
    "                    # You can add enhanced motion task here similar to complaint\n",
    "                    main_task = create_enhanced_complaint_task(case_data, research_context)  # Placeholder\n",
    "                elif document_type == 'memo':\n",
    "                    # You can add enhanced memo task here\n",
    "                    main_task = create_enhanced_complaint_task(case_data, research_context)  # Placeholder\n",
    "                    \n",
    "            elif document_type == 'contract':\n",
    "                contract_data = generate_enhanced_contract_data()\n",
    "                metadata = contract_data\n",
    "                main_task = create_enhanced_contract_task(contract_data, research_context)\n",
    "            \n",
    "            # Create review task\n",
    "            review_task = create_review_task()\n",
    "            \n",
    "            # Create and execute crew\n",
    "            crew = Crew(\n",
    "                agents=[litigation_attorney, contract_specialist, document_reviewer],\n",
    "                tasks=[main_task, review_task],\n",
    "                process=Process.sequential,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Generate document\n",
    "            result = crew.kickoff()\n",
    "            \n",
    "            # Save as TXT file\n",
    "            file_path = file_manager.save_document_txt(\n",
    "                content=str(result),\n",
    "                doc_type=document_type + 's',  # Pluralize for directory name\n",
    "                metadata=metadata\n",
    "            )\n",
    "            \n",
    "            generated_files.append(file_path)\n",
    "            print(f\"‚úì {document_type.capitalize()} {i+1} generated and saved\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating {document_type} {i+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return generated_files\n",
    "\n",
    "def generate_document_dataset(output_dir=\"legal_documents\", counts=None):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive dataset of legal documents as TXT files\n",
    "    \"\"\"\n",
    "    \n",
    "    if counts is None:\n",
    "        counts = {\n",
    "            'complaint': 3,\n",
    "            'contract': 3,\n",
    "            'motion': 2,\n",
    "            'memo': 2\n",
    "        }\n",
    "    \n",
    "    # Update file manager base directory\n",
    "    global file_manager\n",
    "    file_manager = LegalDocumentFileManager(output_dir)\n",
    "    \n",
    "    all_files = []\n",
    "    \n",
    "    for doc_type, count in counts.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Generating {count} {doc_type}(s)...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        files = generate_legal_document_with_research(doc_type, count, use_research=True)\n",
    "        all_files.extend(files)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"GENERATION COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"‚úì Total files generated: {len(all_files)}\")\n",
    "    print(f\"‚úì Saved in directory: {output_dir}\")\n",
    "    \n",
    "    # Show file counts\n",
    "    counts = file_manager.get_document_count()\n",
    "    for doc_type, count in counts.items():\n",
    "        print(f\"  {doc_type}: {count} files\")\n",
    "    \n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the system with a single document\n",
    "print(\"Testing document generation...\")\n",
    "print(\"Current directory structure:\")\n",
    "print(file_manager.get_document_count())\n",
    "\n",
    "# Generate one complaint as a test\n",
    "print(\"\\nGenerating test complaint...\")\n",
    "test_files = generate_legal_document_with_research('complaint', 1, use_research=True)\n",
    "\n",
    "if test_files:\n",
    "    print(f\"\\n‚úì Test file generated: {test_files[0]}\")\n",
    "\n",
    "    # Read and display first 1000 characters\n",
    "    with open(test_files[0], 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        print(\"\\nFirst 1000 characters of generated document:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(content[:1000] + \"...\" if len(content) > 1000 else content)\n",
    "else:\n",
    "    print(\"‚ùå Test generation failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a contract example\n",
    "print(\"Generating test contract...\")\n",
    "contract_files = generate_legal_document_with_research('contract', 1, use_research=True)\n",
    "\n",
    "if contract_files:\n",
    "    print(f\"\\n‚úì Contract file generated: {contract_files[0]}\")\n",
    "    \n",
    "    # Show file structure\n",
    "    print(\"\\nCurrent file structure:\")\n",
    "    counts = file_manager.get_document_count()\n",
    "    for doc_type, count in counts.items():\n",
    "        print(f\"  {doc_type}: {count} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Complete Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a full dataset\n",
    "# Adjust counts as needed for your training requirements\n",
    "\n",
    "dataset_config = {\n",
    "    'complaint': 3,    # 3 civil complaints\n",
    "    'contract': 3,     # 3 various contracts\n",
    "    'motion': 3,        # 3 legal motions\n",
    "    'memo': 3           # 3 legal memos\n",
    "}\n",
    "\n",
    "print(\"Starting full dataset generation...\")\n",
    "print(f\"Total documents to generate: {sum(dataset_config.values())}\")\n",
    "print(\"‚ö†Ô∏è This will take 15-30 minutes and use significant API credits\")\n",
    "\n",
    "# Uncomment the next line to start full generation\n",
    "all_generated_files = generate_document_dataset(\"legal_training_data\", dataset_config)\n",
    "\n",
    "print(\"\\nTo generate the complete dataset, uncomment the line above.\")\n",
    "print(\"This will create 30 legal documents with web research for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Analysis and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_generated_files(base_dir=\"legal_documents\"):\n",
    "    \"\"\"\n",
    "    Analyze the generated TXT files for training readiness\n",
    "    \"\"\"\n",
    "    \n",
    "    base_path = Path(base_dir)\n",
    "    analysis = {\n",
    "        'total_files': 0,\n",
    "        'total_size_mb': 0,\n",
    "        'by_type': {},\n",
    "        'avg_length': {},\n",
    "        'file_list': []\n",
    "    }\n",
    "    \n",
    "    if not base_path.exists():\n",
    "        print(f\"Directory {base_dir} does not exist\")\n",
    "        return analysis\n",
    "    \n",
    "    for subdir in base_path.iterdir():\n",
    "        if subdir.is_dir() and subdir.name not in ['metadata', 'templates']:\n",
    "            doc_type = subdir.name\n",
    "            txt_files = list(subdir.glob('*.txt'))\n",
    "            \n",
    "            if txt_files:\n",
    "                total_length = 0\n",
    "                for txt_file in txt_files:\n",
    "                    file_size = txt_file.stat().st_size\n",
    "                    analysis['total_size_mb'] += file_size / (1024 * 1024)\n",
    "                    \n",
    "                    with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        total_length += len(content)\n",
    "                    \n",
    "                    analysis['file_list'].append(str(txt_file))\n",
    "                \n",
    "                analysis['by_type'][doc_type] = len(txt_files)\n",
    "                analysis['avg_length'][doc_type] = total_length // len(txt_files) if txt_files else 0\n",
    "                analysis['total_files'] += len(txt_files)\n",
    "    \n",
    "    # Print analysis\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DATASET ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total files: {analysis['total_files']}\")\n",
    "    print(f\"Total size: {analysis['total_size_mb']:.2f} MB\")\n",
    "    \n",
    "    print(\"\\nDocument types:\")\n",
    "    for doc_type, count in analysis['by_type'].items():\n",
    "        avg_chars = analysis['avg_length'].get(doc_type, 0)\n",
    "        print(f\"  {doc_type}: {count} files, avg {avg_chars:,} characters\")\n",
    "    \n",
    "    print(\"\\nFirst 5 generated files:\")\n",
    "    for i, file_path in enumerate(analysis['file_list'][:5]):\n",
    "        print(f\"  {i+1}. {Path(file_path).name}\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze current files\n",
    "current_analysis = analyze_generated_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Export Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_manifest(base_dir=\"legal_documents\"):\n",
    "    \"\"\"\n",
    "    Create a manifest file for training data\n",
    "    \"\"\"\n",
    "    \n",
    "    base_path = Path(base_dir)\n",
    "    manifest = []\n",
    "    \n",
    "    for subdir in base_path.iterdir():\n",
    "        if subdir.is_dir() and subdir.name not in ['metadata', 'templates']:\n",
    "            doc_type = subdir.name\n",
    "            \n",
    "            for txt_file in subdir.glob('*.txt'):\n",
    "                # Find corresponding metadata\n",
    "                metadata_file = base_path / 'metadata' / f\"{txt_file.stem}_metadata.json\"\n",
    "                \n",
    "                metadata = {}\n",
    "                if metadata_file.exists():\n",
    "                    with open(metadata_file, 'r') as f:\n",
    "                        metadata = json.load(f)\n",
    "                \n",
    "                manifest.append({\n",
    "                    'file_path': str(txt_file.relative_to(base_path)),\n",
    "                    'document_type': doc_type.rstrip('s'),  # Remove plural\n",
    "                    'file_size': txt_file.stat().st_size,\n",
    "                    'metadata_file': str(metadata_file.relative_to(base_path)) if metadata_file.exists() else None,\n",
    "                    'metadata': metadata\n",
    "                })\n",
    "    \n",
    "    # Save manifest\n",
    "    manifest_path = base_path / 'training_manifest.json'\n",
    "    with open(manifest_path, 'w') as f:\n",
    "        json.dump(manifest, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"‚úì Training manifest created: {manifest_path}\")\n",
    "    print(f\"  Total entries: {len(manifest)}\")\n",
    "    \n",
    "    return manifest\n",
    "\n",
    "def prepare_for_model_training(base_dir=\"legal_documents\"):\n",
    "    \"\"\"\n",
    "    Prepare the complete dataset for model training\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Preparing dataset for model training...\")\n",
    "    \n",
    "    # Create manifest\n",
    "    manifest = create_training_manifest(base_dir)\n",
    "    \n",
    "    # Create summary statistics\n",
    "    analysis = analyze_generated_files(base_dir)\n",
    "    \n",
    "    # Create README for the dataset\n",
    "    readme_content = f\"\"\"# Legal Document Training Dataset\n",
    "\n",
    "Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Dataset Overview\n",
    "- Total files: {analysis['total_files']}\n",
    "- Total size: {analysis['total_size_mb']:.2f} MB\n",
    "- Document types: {', '.join(analysis['by_type'].keys())}\n",
    "\n",
    "## File Structure\n",
    "```\n",
    "{base_dir}/\n",
    "‚îú‚îÄ‚îÄ complaints/     # Civil complaint documents\n",
    "‚îú‚îÄ‚îÄ contracts/      # Various contract types\n",
    "‚îú‚îÄ‚îÄ motions/        # Legal motions and briefs\n",
    "‚îú‚îÄ‚îÄ memos/          # Legal memoranda\n",
    "‚îú‚îÄ‚îÄ metadata/       # JSON metadata for each document\n",
    "‚îî‚îÄ‚îÄ training_manifest.json  # Complete file manifest\n",
    "```\n",
    "\n",
    "## Document Types and Counts\n",
    "\"\"\" + \"\\n\".join([f\"- {doc_type}: {count} files\" for doc_type, count in analysis['by_type'].items()]) + f\"\"\"\n",
    "\n",
    "## Usage Notes\n",
    "- All documents are synthetic and generated for training purposes\n",
    "- Each document includes disclaimers indicating synthetic nature\n",
    "- Metadata files contain case/contract details used for generation\n",
    "- Documents follow realistic legal formatting and structure\n",
    "\n",
    "## Model Training Recommendations\n",
    "1. Use document type classification as initial task\n",
    "2. Implement named entity recognition for legal entities\n",
    "3. Train on legal language patterns and clause identification\n",
    "4. Consider fine-tuning for specific legal document generation\n",
    "\n",
    "## Disclaimer\n",
    "These documents are entirely synthetic and created for AI training purposes.\n",
    "They should not be used for actual legal proceedings or advice.\n",
    "\"\"\"\n",
    "    \n",
    "    readme_path = Path(base_dir) / 'README.md'\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    print(f\"‚úì README created: {readme_path}\")\n",
    "    print(f\"\\nüéâ Dataset ready for model training in: {base_dir}\")\n",
    "    \n",
    "    return {\n",
    "        'manifest': manifest,\n",
    "        'analysis': analysis,\n",
    "        'base_dir': base_dir\n",
    "    }\n",
    "\n",
    "# Prepare current dataset\n",
    "if file_manager.get_document_count():\n",
    "    training_data = prepare_for_model_training()\n",
    "    print(\"\\n‚úÖ Your legal document dataset is ready for model training!\")\n",
    "else:\n",
    "    print(\"\\nüìù Generate some documents first to create the training dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "### Setup Requirements:\n",
    "1. **OpenAI API Key**: Set `OPENAI_API_KEY` in your environment\n",
    "2. **Serper API Key**: Get free key at https://serper.dev/ and set `SERPER_API_KEY`\n",
    "3. **Install packages**: Run the pip install command above\n",
    "\n",
    "### Key Features:\n",
    "1. **Web Research**: Uses Serper to find current legal document templates\n",
    "2. **TXT Output**: Saves documents as individual text files with metadata\n",
    "3. **File Management**: Organized directory structure for different document types\n",
    "4. **Training Ready**: Includes manifest and analysis for ML training\n",
    "\n",
    "### Generated File Structure:\n",
    "```\n",
    "legal_documents/\n",
    "‚îú‚îÄ‚îÄ complaints/          # Civil complaints\n",
    "‚îú‚îÄ‚îÄ contracts/           # Various contracts\n",
    "‚îú‚îÄ‚îÄ motions/            # Legal motions\n",
    "‚îú‚îÄ‚îÄ memos/              # Legal memoranda\n",
    "‚îú‚îÄ‚îÄ metadata/           # JSON metadata files\n",
    "‚îú‚îÄ‚îÄ training_manifest.json\n",
    "‚îî‚îÄ‚îÄ README.md\n",
    "```\n",
    "\n",
    "### For Your Model Training:\n",
    "1. Each document is a separate TXT file\n",
    "2. Metadata available in JSON format\n",
    "3. Manifest file lists all documents with metadata\n",
    "4. Documents include proper legal formatting\n",
    "5. Clear synthetic disclaimers included\n",
    "\n",
    "### Customization:\n",
    "- Modify `dataset_config` to change document counts\n",
    "- Adjust `case_types` and `contract_types` for specific domains\n",
    "- Update research queries for different legal areas\n",
    "- Change file organization in `LegalDocumentFileManager`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
